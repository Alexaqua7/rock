from torch import nn
import os
from tqdm import tqdm
import wandb
import torch
from sklearn.metrics import confusion_matrix, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from utils.custom_decorator import with_torch_no_grad

def train(model, optimizer, train_loader, val_loader, scheduler, device, class_names, criterion=nn.CrossEntropyLoss(), epochs=15, best_score=0, cur_epoch=1, experiment_name="base", folder_path = "base", accumulation_steps=1):
    model.to(device)
    best_model = None
    save_path = os.path.join(folder_path, f"{experiment_name}-best.pth")

    for epoch in range(cur_epoch, epochs + 1):
        model.train()
        train_loss = []
        optimizer.zero_grad()
        progress_bar = tqdm(enumerate(iter(train_loader)), total=len(train_loader), desc=f"Epoch {epoch}/{epochs}")
        for step, (imgs, labels) in progress_bar:
            imgs = imgs.float().to(device)
            labels = labels.to(device).long()

            output = model(imgs)
            loss = criterion(output, labels)
            loss = loss / accumulation_steps
            loss.backward()
            if (step+1) % accumulation_steps == 0 or (step+1) == len(train_loader):
                optimizer.step()
                optimizer.zero_grad()

            train_loss.append(loss.item())
            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)

        _val_loss, _val_score, class_f1_dict, wandb_cm = validation(model, criterion, val_loader, device, class_names)

        log_data = {
            'epoch': epoch,
            'train_loss': sum(train_loss) / len(train_loss) * accumulation_steps,
            'val_loss': _val_loss * accumulation_steps,
            'val_macro_f1': _val_score,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'confusion_matrix': wandb_cm
        }
        log_data.update(class_f1_dict)
        wandb.log(log_data)

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
            'best_score': best_score
        }
        torch.save(checkpoint, os.path.join(folder_path, f'{experiment_name}-epoch_{epoch}.pth'))
        print(f"Checkpoint at epoch {epoch} saved â†’ {experiment_name}-epoch_{epoch}.pth")

        prev_path = os.path.join(folder_path, f'{experiment_name}-epoch_{epoch-1}.pth')
        if epoch > 1 and os.path.exists(prev_path):
            os.remove(prev_path)
            print(f"Previous checkpoint deleted â†’ {prev_path}")

        if scheduler is not None:
            scheduler.step()

        if best_score < _val_score:
            best_score = _val_score
            best_model = model
            checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
            'best_score': best_score
        }
            torch.save(checkpoint, save_path)
            print(f"Best model saved (epoch {epoch}, F1={_val_score:.4f}) â†’ {save_path}")

    return best_model

def hard_negative_train(model, optimizer, train_loader, val_loader, scheduler, device, class_names, criterion=nn.CrossEntropyLoss(reduction='none'), hard_negative_miner=None, best_score=0, epochs=15, cur_epoch=1, experiment_name="base", folder_path="base", accumulation_steps=1):
    model.to(device)
    
    best_model = None
    save_path = os.path.join(folder_path, f"{experiment_name}-best.pth")
    
    for epoch in range(cur_epoch, epochs + 1):
        model.train()
        train_loss = []
        optimizer.zero_grad()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•œ ë°°ì¹˜ ì†ì‹¤ ë° ì¸ë±ìŠ¤ ì €ì¥ì†Œ
        accumulated_indices = []
        accumulated_losses = []
        
        progress_bar = tqdm(enumerate(iter(train_loader)), total=len(train_loader), desc=f"Epoch {epoch}/{epochs}")
        for step, batch in progress_bar:
            if len(batch) == 3:  # ì¸ë±ìŠ¤ í¬í•¨í•˜ëŠ” ê²½ìš°
                imgs, labels, indices = batch
            else:
                imgs, labels = batch
                indices = None
            
            imgs = imgs.float().to(device)
            labels = labels.to(device).long()

            output = model(imgs)
            
            # ìƒ˜í”Œë³„ ì†ì‹¤ê°’ ê³„ì‚° - ì›ë˜ ìŠ¤ì¼€ì¼ ìœ ì§€ (accumulation_stepsë¡œ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
            sample_losses = criterion(output, labels)
            loss = sample_losses.mean() / accumulation_steps  # ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•´ì„œë§Œ ë‚˜ëˆ”

            # ì—­ì „íŒŒ ìˆ˜í–‰
            loss.backward()
            
            # í˜„ì¬ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ê³¼ ì¸ë±ìŠ¤ ì €ì¥
            if indices is not None:
                accumulated_indices.extend(indices.cpu().numpy())
                accumulated_losses.extend(sample_losses.detach().cpu())

            train_loss.append(loss.item() * accumulation_steps)  # ì›ë˜ ì†ì‹¤ê°’ ì €ì¥ (scaling ì œê±°)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ë° í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë„ˆ ì—…ë°ì´íŠ¸
            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                optimizer.step()
                optimizer.zero_grad()
                
                # ëˆ„ì ëœ ë°°ì¹˜ë“¤ì— ëŒ€í•œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë„ˆ ì—…ë°ì´íŠ¸
                if hard_negative_miner is not None and accumulated_indices:
                    hard_negative_miner.update(accumulated_indices, accumulated_losses)
                
                # ëˆ„ì  ì €ì¥ì†Œ ì´ˆê¸°í™”
                accumulated_indices = []
                accumulated_losses = []
            
            # ì§„í–‰ìƒí™© í‘œì‹œ
            progress_bar.set_postfix(loss=loss.item() * accumulation_steps)  # ì›ë˜ ì†ì‹¤ê°’ í‘œì‹œ

        # ê²€ì¦ ë‹¨ê³„
        _val_loss, _val_score, class_f1_dict, wandb_cm = hard_negative_validation(model, criterion, val_loader, device, class_names)

        log_data = {
            'epoch': epoch,
            'train_loss': sum(train_loss) / len(train_loss),
            'val_loss': _val_loss,
            'val_macro_f1': _val_score,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'confusion_matrix': wandb_cm
        }
        log_data.update(class_f1_dict)
        wandb.log(log_data)

        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
            'best_score': best_score
        }
        torch.save(checkpoint, os.path.join(folder_path, f'{experiment_name}-epoch_{epoch}.pth'))
        print(f"Checkpoint at epoch {epoch} saved â†’ {experiment_name}-epoch_{epoch}.pth")

        prev_path = os.path.join(folder_path, f'{experiment_name}-epoch_{epoch-1}.pth')
        if epoch > 1 and os.path.exists(prev_path):
            os.remove(prev_path)
            print(f"Previous checkpoint deleted â†’ {prev_path}")

        if scheduler is not None:
            scheduler.step()

        if best_score < _val_score:
            best_score = _val_score
            best_model = model
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
                'best_score': best_score
            }
            torch.save(checkpoint, save_path)
            print(f"Best model saved (epoch {epoch}, F1={_val_score:.4f}) â†’ {save_path}")

    return best_model

@with_torch_no_grad
def validation(model, criterion, val_loader, device, class_names):
    model.eval()
    val_loss = []
    preds, true_labels = [], []

    for imgs, labels in tqdm(iter(val_loader)):
        imgs = imgs.float().to(device)
        labels = labels.to(device).long()

        pred = model(imgs)
        loss = criterion(pred, labels)

        preds += pred.argmax(1).detach().cpu().numpy().tolist()
        true_labels += labels.detach().cpu().numpy().tolist()
        val_loss.append(loss.item())

    _val_loss = np.mean(val_loss)
    _val_score = f1_score(true_labels, preds, average='macro')

    # ğŸ”¹ classë³„ F1 (í´ë˜ìŠ¤ëª… í¬í•¨)
    class_f1 = f1_score(true_labels, preds, average=None)
    class_f1_dict = {f'{class_names[i]}_f1': f for i, f in enumerate(class_f1)}

    # ğŸ”¹ confusion matrix
    cm = confusion_matrix(true_labels, preds)
    fig, ax = plt.subplots(figsize=(7, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                xticklabels=class_names, yticklabels=class_names)
    ax.set_xlabel("Predicted Labels")
    ax.set_ylabel("True Labels")
    ax.set_title("Confusion Matrix")
    plt.tight_layout()
    
    # ğŸ”¹ wandb ì´ë¯¸ì§€ë¡œ ì €ì¥
    wandb_cm = wandb.Image(fig)
    plt.close(fig)

    return _val_loss, _val_score, class_f1_dict, wandb_cm

@with_torch_no_grad
def hard_negative_validation(model, criterion, val_loader, device, class_names):
    model.eval()
    val_loss = []
    preds, true_labels = [], []

    for batch in tqdm(iter(val_loader)):
        if len(batch) == 3:  # ì¸ë±ìŠ¤ í¬í•¨í•˜ëŠ” ê²½ìš°
            imgs, labels, _ = batch
        else:
            imgs, labels = batch
        
        imgs = imgs.float().to(device)
        labels = labels.to(device).long()

        pred = model(imgs)
        sample_losses = criterion(pred, labels)  # ìƒ˜í”Œë³„ ì†ì‹¤ê°’ ê³„ì‚°
        loss = sample_losses.mean()  # í‰ê·  ì†ì‹¤ê°’ ê³„ì‚°

        preds += pred.argmax(1).detach().cpu().numpy().tolist()
        true_labels += labels.detach().cpu().numpy().tolist()
        val_loss.append(loss.item())

    _val_loss = np.mean(val_loss)
    _val_score = f1_score(true_labels, preds, average='macro')

    # ğŸ”¹ classë³„ F1 (í´ë˜ìŠ¤ëª… í¬í•¨)
    class_f1 = f1_score(true_labels, preds, average=None)
    class_f1_dict = {f'{class_names[i]}_f1': f for i, f in enumerate(class_f1)}

    # ğŸ”¹ confusion matrix
    cm = confusion_matrix(true_labels, preds)
    fig, ax = plt.subplots(figsize=(7, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                xticklabels=class_names, yticklabels=class_names)
    ax.set_xlabel("Predicted Labels")
    ax.set_ylabel("True Labels")
    ax.set_title("Confusion Matrix")
    plt.tight_layout()
    
    # ğŸ”¹ wandb ì´ë¯¸ì§€ë¡œ ì €ì¥
    wandb_cm = wandb.Image(fig)
    plt.close(fig)

    return _val_loss, _val_score, class_f1_dict, wandb_cm

def progressive_hard_negative_train(model, optimizer, train_dataset, val_loader, scheduler, device, 
                                  class_names, labels, num_classes, batch_size, accumulation_steps,
                                  progressive_scheduler, hard_negative_miner=None, 
                                  criterion=nn.CrossEntropyLoss(reduction='none'), 
                                  best_score=0, epochs=15, cur_epoch=1, 
                                  experiment_name="progressive", folder_path="progressive",
                                  num_workers=16, prefetch_factor=4, pin_memory=True):
    """
    Progressive Hard Negative Training í•¨ìˆ˜
    
    Args:
        model: í›ˆë ¨í•  ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì €
        train_dataset: í›ˆë ¨ ë°ì´í„°ì…‹ (ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•´ì•¼ í•¨)
        val_loader: ê²€ì¦ ë°ì´í„°ë¡œë”
        scheduler: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        device: ë””ë°”ì´ìŠ¤
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        labels: í›ˆë ¨ ë°ì´í„°ì˜ ë ˆì´ë¸” (numpy array)
        num_classes: í´ë˜ìŠ¤ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        accumulation_steps: gradient accumulation ìŠ¤í…
        progressive_scheduler: ProgressiveScheduler ì¸ìŠ¤í„´ìŠ¤
        hard_negative_miner: HardNegativeMiner ì¸ìŠ¤í„´ìŠ¤
        criterion: ì†ì‹¤ í•¨ìˆ˜
        best_score: ì´ˆê¸° ìµœê³  ì ìˆ˜
        epochs: ì´ ì—í¬í¬ ìˆ˜
        cur_epoch: ì‹œì‘ ì—í¬í¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
        folder_path: ëª¨ë¸ ì €ì¥ í´ë”
        num_workers: ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜
        prefetch_factor: í”„ë¦¬í˜ì¹˜ íŒ©í„°
        pin_memory: ë©”ëª¨ë¦¬ í•€ ì—¬ë¶€
    """
    model.to(device)
    
    best_model = None
    save_path = os.path.join(folder_path, f"{experiment_name}-best.pth")
    
    for epoch in range(cur_epoch, epochs + 1):
        print(f"\n{'='*50}")
        print(f"Starting Epoch {epoch}/{epochs}")
        print(f"{'='*50}")
        
        # Progressive samplerë¡œ ìƒˆë¡œìš´ train loader ìƒì„±
        from utils.sampling import create_progressive_train_loader  # ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ import
        
        train_loader, batch_sampler = create_progressive_train_loader(
            train_dataset=train_dataset,
            hard_negative_miner=hard_negative_miner,
            labels=labels,
            num_classes=num_classes,
            batch_size=batch_size,
            accumulation_steps=accumulation_steps,
            progressive_scheduler=progressive_scheduler,
            current_epoch=epoch - 1,  # 0-based indexing
            num_workers=num_workers,
            prefetch_factor=prefetch_factor,
            pin_memory=pin_memory
        )
        
        # í˜„ì¬ epochì˜ hard negative ratio ë¡œê¹…
        current_ratio = progressive_scheduler.get_ratio(epoch - 1)
        print(f"Current Hard Negative Ratio: {current_ratio:.3f}")
        
        model.train()
        train_loss = []
        optimizer.zero_grad()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•œ ë°°ì¹˜ ì†ì‹¤ ë° ì¸ë±ìŠ¤ ì €ì¥ì†Œ
        accumulated_indices = []
        accumulated_losses = []
        
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), 
                           desc=f"Epoch {epoch}/{epochs} (HN_ratio: {current_ratio:.3f})")
        
        for step, batch in progress_bar:
            if len(batch) == 3:  # ì¸ë±ìŠ¤ í¬í•¨í•˜ëŠ” ê²½ìš°
                imgs, labels_batch, indices = batch
            else:
                imgs, labels_batch = batch
                indices = None
            
            imgs = imgs.float().to(device)
            labels_batch = labels_batch.to(device).long()

            output = model(imgs)
            
            # ìƒ˜í”Œë³„ ì†ì‹¤ê°’ ê³„ì‚° - ì›ë˜ ìŠ¤ì¼€ì¼ ìœ ì§€ (accumulation_stepsë¡œ ë‚˜ëˆ„ì§€ ì•ŠìŒ)
            sample_losses = criterion(output, labels_batch)
            loss = sample_losses.mean() / accumulation_steps  # ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•´ì„œë§Œ ë‚˜ëˆ”

            # ì—­ì „íŒŒ ìˆ˜í–‰
            loss.backward()
            
            # í˜„ì¬ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ê³¼ ì¸ë±ìŠ¤ ì €ì¥
            if indices is not None:
                accumulated_indices.extend(indices.cpu().numpy())
                accumulated_losses.extend(sample_losses.detach().cpu())

            train_loss.append(loss.item() * accumulation_steps)  # ì›ë˜ ì†ì‹¤ê°’ ì €ì¥ (scaling ì œê±°)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ë° í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë„ˆ ì—…ë°ì´íŠ¸
            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                optimizer.step()
                optimizer.zero_grad()
                
                # ëˆ„ì ëœ ë°°ì¹˜ë“¤ì— ëŒ€í•œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë„ˆ ì—…ë°ì´íŠ¸
                if hard_negative_miner is not None and accumulated_indices:
                    hard_negative_miner.update(accumulated_indices, accumulated_losses)
                
                # ëˆ„ì  ì €ì¥ì†Œ ì´ˆê¸°í™”
                accumulated_indices = []
                accumulated_losses = []
            
            # ì§„í–‰ìƒí™© í‘œì‹œ
            progress_bar.set_postfix({
                'loss': f"{loss.item() * accumulation_steps:.4f}",
                'hn_ratio': f"{current_ratio:.3f}"
            })

        # ê²€ì¦ ë‹¨ê³„
        _val_loss, _val_score, class_f1_dict, wandb_cm = hard_negative_validation(
            model, criterion, val_loader, device, class_names
        )

        # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë„ˆ ìƒíƒœ ë¡œê¹…
        hard_negative_stats = {}
        if hard_negative_miner is not None:
            hard_negative_stats = {
                'hard_negative_count': len(hard_negative_miner.hard_negative_indices),
                'hard_negative_ratio': current_ratio,
                'hard_negative_memory_usage': len(hard_negative_miner.hard_negative_indices) / hard_negative_miner.memory_size
            }

        log_data = {
            'epoch': epoch,
            'train_loss': sum(train_loss) / len(train_loss),
            'val_loss': _val_loss,
            'val_macro_f1': _val_score,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'confusion_matrix': wandb_cm,
            **hard_negative_stats  # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ í†µê³„ ì¶”ê°€
        }
        log_data.update(class_f1_dict)
        wandb.log(log_data)

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,
            'best_score': best_score,
            'progressive_scheduler_state': {
                'initial_ratio': progressive_scheduler.initial_ratio,
                'final_ratio': progressive_scheduler.final_ratio,
                'total_epochs': progressive_scheduler.total_epochs,
                'schedule_type': progressive_scheduler.schedule_type
            },
            'hard_negative_miner_state': {
                'hard_negative_indices': list(hard_negative_miner.hard_negative_indices) if hard_negative_miner else [],
                'hard_negative_scores': hard_negative_miner.hard_negative_scores if hard_negative_miner else {},
                'memory_size': hard_negative_miner.memory_size if hard_negative_miner else 0
            }
        }
        
        epoch_checkpoint_path = os.path.join(folder_path, f'{experiment_name}-epoch_{epoch}.pth')
        torch.save(checkpoint, epoch_checkpoint_path)
        print(f"Checkpoint at epoch {epoch} saved â†’ {experiment_name}-epoch_{epoch}.pth")

        # ì´ì „ ì—í¬í¬ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
        prev_path = os.path.join(folder_path, f'{experiment_name}-epoch_{epoch-1}.pth')
        if epoch > 1 and os.path.exists(prev_path):
            os.remove(prev_path)
            print(f"Previous checkpoint deleted â†’ {prev_path}")

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        if scheduler is not None:
            scheduler.step()

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if best_score < _val_score:
            best_score = _val_score
            best_model = model
            checkpoint['best_score'] = best_score
            torch.save(checkpoint, save_path)
            print(f"Best model saved (epoch {epoch}, F1={_val_score:.4f}) â†’ {save_path}")

        print(f"Epoch {epoch} Summary:")
        print(f"  - Train Loss: {sum(train_loss) / len(train_loss):.4f}")
        print(f"  - Val Loss: {_val_loss:.4f}")
        print(f"  - Val F1: {_val_score:.4f}")
        print(f"  - Hard Negative Ratio: {current_ratio:.3f}")
        if hard_negative_miner:
            print(f"  - Hard Negative Count: {len(hard_negative_miner.hard_negative_indices)}")

    return best_model


def load_progressive_checkpoint(checkpoint_path, model, optimizer, scheduler, 
                              progressive_scheduler, hard_negative_miner, device):
    """
    Progressive training ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    """
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler is not None and checkpoint.get('scheduler_state_dict'):
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    # Progressive scheduler ìƒíƒœ ë³µì›
    if 'progressive_scheduler_state' in checkpoint:
        scheduler_state = checkpoint['progressive_scheduler_state']
        progressive_scheduler.initial_ratio = scheduler_state['initial_ratio']
        progressive_scheduler.final_ratio = scheduler_state['final_ratio']
        progressive_scheduler.total_epochs = scheduler_state['total_epochs']
        progressive_scheduler.schedule_type = scheduler_state['schedule_type']
    
    # Hard negative miner ìƒíƒœ ë³µì›
    if hard_negative_miner and 'hard_negative_miner_state' in checkpoint:
        miner_state = checkpoint['hard_negative_miner_state']
        hard_negative_miner.hard_negative_indices = set(miner_state['hard_negative_indices'])
        hard_negative_miner.hard_negative_scores = miner_state['hard_negative_scores']
        hard_negative_miner.memory_size = miner_state['memory_size']
    
    return checkpoint['epoch'], checkpoint['best_score']