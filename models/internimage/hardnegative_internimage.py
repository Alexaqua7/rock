import glob
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, Sampler
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch
from PIL import Image
from transformers import AutoModelForImageClassification, CLIPImageProcessor
import numpy as np
import os
import random
from tqdm import tqdm
from torch.cuda.amp import autocast as amp_autocast, GradScaler
from contextlib import suppress
import time
from albumentations.core.transforms_interface import ImageOnlyTransform
import wandb
from sklearn.metrics import f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
from sklearn.model_selection import StratifiedKFold
from torch import nn
import cv2

CFG = {
    'IMG_SIZE': 384,
    'EPOCHS': 20,
    'LEARNING_RATE': 8e-5,
    'BATCH_SIZE': 8,
    'SEED': 41,
    'AMP_TYPE': 'bfloat16',  # ì˜ˆì‹œë¡œ ì¶”ê°€
    'AMP_OPT_LEVEL': 'O1',  # ì˜ˆì‹œë¡œ ì¶”ê°€
    'kFold': 5,
    'HARD_NEGATIVE_MEMORY_SIZE': 1000,
    'HARD_NEGATIVE_RATIO': 0.2,
    'TRAIN': {
        'ACCUMULATION_STEPS': 32, # ì˜ˆì‹œë¡œ ì¶”ê°€
        'CLIP_GRAD': None # ì˜ˆì‹œë¡œ ì¶”ê°€
    }
}

def create_weighted_sampler(labels):
    """ê° í´ë˜ìŠ¤ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ëŠ” WeightedRandomSampler ìƒì„±"""
    class_counts = np.bincount(labels)
    class_weights = 1. / class_counts
    weights = class_weights[labels]
    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)
    return sampler

class HardNegativeMiner:
    def __init__(self, dataset_size, memory_size=1000):
        self.hard_negative_indices = set()
        self.hard_negative_scores = {}  # ê° ìƒ˜í”Œì˜ ì†ì‹¤ê°’ ì €ì¥
        self.memory_size = memory_size
        self.dataset_size = dataset_size
    
    def update(self, indices, losses):
        """
        ì†ì‹¤ê°’ì´ ë†’ì€ ìƒ˜í”Œë“¤ì„ í•˜ë“œ ë„¤ê±°í‹°ë¸Œë¡œ ì¶”ê°€
        
        Args:
            indices: ìƒ˜í”Œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            losses: ê° ìƒ˜í”Œì˜ ì†ì‹¤ê°’ ë¦¬ìŠ¤íŠ¸
        """
        # ì¸ë±ìŠ¤ì™€ ì†ì‹¤ê°’ ë§¤í•‘
        for idx, loss in zip(indices, losses):
            self.hard_negative_scores[idx] = loss.item()
        
        # ì†ì‹¤ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ memory_sizeê°œë§Œ ìœ ì§€
        sorted_items = sorted(self.hard_negative_scores.items(), 
                              key=lambda x: x[1], reverse=True)
        
        if len(sorted_items) > self.memory_size:
            sorted_items = sorted_items[:self.memory_size]
        
        # ìƒˆë¡œìš´ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ì¸ë±ìŠ¤ ì§‘í•© êµ¬ì„±
        self.hard_negative_indices = set([idx for idx, _ in sorted_items])
        self.hard_negative_scores = {idx: loss for idx, loss in sorted_items}
    
    def get_hard_negatives(self, count):
        """
        í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œì—ì„œ 'count'ë§Œí¼ ìƒ˜í”Œë§
        
        Args:
            count: í•„ìš”í•œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ìˆ˜
            
        Returns:
            ì„ íƒëœ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if not self.hard_negative_indices:
            # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ëœë¤ ì¸ë±ìŠ¤ ë°˜í™˜
            return random.sample(range(self.dataset_size), min(count, self.dataset_size))
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìƒ˜í”Œë§ì„ ìœ„í•œ ì¤€ë¹„
        indices = list(self.hard_negative_indices)
        weights = [self.hard_negative_scores[idx] for idx in indices]
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = np.array(weights)
        weights = weights / np.sum(weights)
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìƒ˜í”Œë§
        if len(indices) <= count:
            return indices
        else:
            return np.random.choice(indices, count, replace=False, p=weights).tolist()

class BalancedHardNegativeBatchSampler(Sampler):
    def __init__(self, dataset_size, batch_size, hard_negative_miner, labels, num_classes, 
                 hard_negative_ratio=0.3, accumulation_steps=1):
        """
        Gradient Accumulationì„ ê³ ë ¤í•œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œê³¼ í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”ŒëŸ¬
        
        Args:
            dataset_size: ë°ì´í„°ì…‹ í¬ê¸°
            batch_size: ê°œë³„ ë°°ì¹˜ í¬ê¸°
            hard_negative_miner: í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤
            labels: ê° ìƒ˜í”Œì˜ í´ë˜ìŠ¤ ë ˆì´ë¸” (numpy array)
            num_classes: í´ë˜ìŠ¤ ìˆ˜
            hard_negative_ratio: effective batchì—ì„œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œì˜ ë¹„ìœ¨ (0~1)
            accumulation_steps: gradient accumulation ìŠ¤í… ìˆ˜
        """
        self.dataset_size = dataset_size
        self.batch_size = batch_size
        self.hard_negative_miner = hard_negative_miner
        self.labels = labels
        self.num_classes = num_classes
        self.hard_negative_ratio = hard_negative_ratio
        self.accumulation_steps = accumulation_steps
        
        # Effective batch size ê³„ì‚°
        self.effective_batch_size = batch_size * accumulation_steps
        
        # Effective batchë‹¹ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ìˆ˜
        self.hard_negative_per_effective_batch = int(self.effective_batch_size * hard_negative_ratio)
        self.random_per_effective_batch = self.effective_batch_size - self.hard_negative_per_effective_batch
        
        # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ êµ¬ì„±
        self.class_indices = [[] for _ in range(num_classes)]
        for idx, label in enumerate(labels):
            self.class_indices[label].append(idx)
        
        # ì—í¬í¬ë‹¹ effective batch ìˆ˜ ê³„ì‚°
        self.num_effective_batches = (dataset_size + self.effective_batch_size - 1) // self.effective_batch_size
        self.num_batches = self.num_effective_batches * accumulation_steps
        
        print(f"Sampler Configuration:")
        print(f"  - Batch size: {batch_size}")
        print(f"  - Accumulation steps: {accumulation_steps}")
        print(f"  - Effective batch size: {self.effective_batch_size}")
        print(f"  - Hard negative per effective batch: {self.hard_negative_per_effective_batch}")
        print(f"  - Random samples per effective batch: {self.random_per_effective_batch}")
        print(f"  - Samples per class per effective batch: {self.random_per_effective_batch // num_classes}")
        
    def _get_balanced_samples_for_effective_batch(self, hard_indices_set, class_positions):
        """Effective batchì— ëŒ€í•´ í´ë˜ìŠ¤ë³„ë¡œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œ ì„ íƒ"""
        samples_per_class = self.random_per_effective_batch // self.num_classes
        remaining_samples = self.random_per_effective_batch % self.num_classes
        
        balanced_indices = []
        
        # ê° í´ë˜ìŠ¤ì—ì„œ samples_per_class ê°œì˜ ìƒ˜í”Œ ì„ íƒ
        for class_idx in range(self.num_classes):
            class_samples_needed = samples_per_class + (1 if class_idx < remaining_samples else 0)
            if class_samples_needed == 0:
                continue
            
            selected_indices = []
            checked_indices = 0
            class_indices = self.class_indices[class_idx]
            
            # ì´ë¯¸ ì„ íƒëœ í•˜ë“œ ë„¤ê±°í‹°ë¸Œì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒ˜í”Œ ì„ íƒ
            while len(selected_indices) < class_samples_needed and checked_indices < len(class_indices) * 2:
                pos = (class_positions[class_idx] + checked_indices) % len(class_indices)
                idx = class_indices[pos]
                checked_indices += 1
                
                if idx not in hard_indices_set:
                    selected_indices.append(idx)
            
            # ì¶©ë¶„í•œ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ í•˜ë“œ ë„¤ê±°í‹°ë¸Œì™€ ì¤‘ë³µ í—ˆìš©
            if len(selected_indices) < class_samples_needed:
                additional_needed = class_samples_needed - len(selected_indices)
                # ë¬´ì‘ìœ„ë¡œ í•´ë‹¹ í´ë˜ìŠ¤ì—ì„œ ì¶”ê°€ ìƒ˜í”Œ ì„ íƒ
                additional_candidates = [idx for idx in class_indices if idx not in selected_indices]
                if additional_candidates:
                    additional_indices = random.sample(
                        additional_candidates, 
                        min(additional_needed, len(additional_candidates))
                    )
                    selected_indices.extend(additional_indices)
                
                # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì¤‘ë³µ í—ˆìš©í•˜ì—¬ ì±„ì›€
                while len(selected_indices) < class_samples_needed:
                    selected_indices.append(random.choice(class_indices))
            
            # ë‹¤ìŒ effective batchë¥¼ ìœ„í•´ í´ë˜ìŠ¤ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
            class_positions[class_idx] = (class_positions[class_idx] + len(selected_indices)) % len(class_indices)
            balanced_indices.extend(selected_indices)
        
        return balanced_indices
    
    def __iter__(self):
        # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ì„ê¸°
        for class_idx in range(self.num_classes):
            random.shuffle(self.class_indices[class_idx])
        
        # ê° í´ë˜ìŠ¤ë³„ í˜„ì¬ ì¸ë±ìŠ¤ ìœ„ì¹˜ ì´ˆê¸°í™”
        class_positions = [0] * self.num_classes
        
        for effective_batch_idx in range(self.num_effective_batches):
            # Effective batchì— ëŒ€í•´ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ì„ íƒ
            hard_indices = self.hard_negative_miner.get_hard_negatives(self.hard_negative_per_effective_batch)
            hard_indices_set = set(hard_indices)
            
            # Effective batchì— ëŒ€í•´ í´ë˜ìŠ¤ë³„ë¡œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œ ì„ íƒ
            balanced_indices = self._get_balanced_samples_for_effective_batch(
                hard_indices_set, class_positions
            )
            
            # ì „ì²´ effective batch ì¸ë±ìŠ¤
            effective_batch_indices = hard_indices + balanced_indices
            random.shuffle(effective_batch_indices)
            
            # Effective batchë¥¼ ê°œë³„ ë°°ì¹˜ë“¤ë¡œ ë¶„í• 
            for step in range(self.accumulation_steps):
                start_idx = step * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(effective_batch_indices))
                
                batch_indices = effective_batch_indices[start_idx:end_idx]
                
                # ë°°ì¹˜ í¬ê¸°ê°€ ë¶€ì¡±í•œ ê²½ìš° íŒ¨ë”© (ë§ˆì§€ë§‰ ë°°ì¹˜ì—ì„œ ë°œìƒ ê°€ëŠ¥)
                while len(batch_indices) < self.batch_size and len(effective_batch_indices) > 0:
                    batch_indices.append(random.choice(effective_batch_indices))
                
                yield batch_indices
    
    def __len__(self):
        return self.num_batches


# ì‚¬ìš© ì˜ˆì‹œë¥¼ ìœ„í•œ ìˆ˜ì •ëœ ë°ì´í„°ë¡œë” ìƒì„± í•¨ìˆ˜
def create_train_loader_with_accumulation(train_dataset, hard_negative_miner, labels, num_classes, 
                                        batch_size, accumulation_steps, hard_negative_ratio=0.2):
    """
    Gradient Accumulationì„ ê³ ë ¤í•œ train loader ìƒì„±
    """
    balanced_batch_sampler = BalancedHardNegativeBatchSampler(
        dataset_size=len(train_dataset),
        batch_size=batch_size,
        hard_negative_miner=hard_negative_miner,
        labels=labels,
        num_classes=num_classes,
        hard_negative_ratio=hard_negative_ratio,
        accumulation_steps=accumulation_steps
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_sampler=balanced_batch_sampler,
        num_workers=16, 
        pin_memory=True,
        prefetch_factor=4
    )
    
    return train_loader

class PadSquare(ImageOnlyTransform):
    def __init__(self, border_mode=0, value=0, always_apply=False, p=1.0):
        super().__init__(always_apply, p)
        self.border_mode = border_mode
        self.value = value

    def apply(self, image, **params):
        h, w, c = image.shape
        max_dim = max(h, w)
        pad_h = max_dim - h
        pad_w = max_dim - w
        top = pad_h // 2
        bottom = pad_h - top
        left = pad_w // 2
        right = pad_w - left
        image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=self.value)
        return image

    def get_transform_init_args_names(self):
        return ("border_mode", "value")

class RandomCenterCrop(ImageOnlyTransform):
    def __init__(self, min_size=75, max_size=200, always_apply=False, p=1.0):
        # ëª…ì‹œì ìœ¼ë¡œ p ê°’ ì „ë‹¬
        super(RandomCenterCrop, self).__init__(always_apply=always_apply, p=p)
        self.min_size = min_size
        self.max_size = max_size
        # ì´ˆê¸°í™” ì‹œ p ê°’ í™•ì¸
    
    def apply(self, img, **params):
        h = np.random.randint(self.min_size, self.max_size)
        w = np.random.randint(self.min_size, self.max_size)
        crop = A.CenterCrop(height=h, width=w, pad_if_needed=True, p=1)
        return crop(image=img)['image']
    
    def get_transform_init_args_names(self):
        return ("min_size", "max_size", "p")
    
    def __str__(self):
        return f"RandomCenterCrop(p={self.p}, min_size={self.min_size}, max_size={self.max_size})"

class CustomDataset(Dataset):
    def __init__(self, img_path_list, label_list, transforms=None, save_images=False, return_index=False):
        self.img_path_list = img_path_list
        self.label_list = label_list
        self.transforms = transforms
        self.save_images = save_images
        self.return_index = return_index  # ì¸ë±ìŠ¤ ë°˜í™˜ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í”Œë˜ê·¸

    def __getitem__(self, index):
        img_path = self.img_path_list[index]
        image = cv2.imread(img_path)
        if self.transforms is not None:
            transformed = self.transforms(image=image)
            image = transformed['image']

            if self.save_images:
                self.save_image(transformed, index)

        if self.label_list is not None:
            label = self.label_list[index]
            if self.return_index:
                return image, label, index  # ì¸ë±ìŠ¤ë„ í•¨ê»˜ ë°˜í™˜
            else:
                return image, label
        else:
            if self.return_index:
                return image, index  # ì¸ë±ìŠ¤ë„ í•¨ê»˜ ë°˜í™˜
            else:
                return image
        
    def __len__(self):
        return len(self.img_path_list)
    
    def save_image(self, transformed, index):
        # í…ì„œì¸ ê²½ìš°
        save_img = transformed['image']
        save_img = cv2.cvtColor(save_img, cv2.COLOR_BGR2RGB)
        if isinstance(save_img, torch.Tensor):
            img_np = save_img.permute(1, 2, 0).cpu().numpy()  # (C, H, W) â†’ (H, W, C)
            img_np = np.clip(img_np * 255.0, 0, 255).astype(np.uint8)  # [0,1] â†’ [0,255] í›„ uint8
        else:
            img_np = transformed['image']
            if img_np.dtype != np.uint8:
                img_np = np.clip(img_np, 0, 255).astype(np.uint8)

        output_dir = "./saved_images"
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, f'transformed_image_{index}.jpg')
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        cv2.imwrite(save_path, img_bgr)
        print(f"ë³€í™˜ëœ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path}")

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

def get_grad_norm(parameters, norm_type=2):
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = [p for p in parameters if p.grad is not None]
    norm_type = float(norm_type)
    if norm_type == torch.inf:
        total_norm = max(p.grad.data.abs().max() for p in parameters)
    else:
        total_norm = torch.norm(
            torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type
        )
    return total_norm

def train_one_epoch(CFG,
                    model,
                    criterion,
                    data_loader,
                    optimizer,
                    epoch,
                    mixup_fn,
                    lr_scheduler,
                    amp_autocast=suppress,
                    loss_scaler=None,
                    model_ema=None,
                    hard_negative_miner=None):
    model.train()
    optimizer.zero_grad()

    num_steps = len(data_loader)
    amp_type = torch.float16 if CFG['AMP_TYPE'] == 'float16' else torch.bfloat16
    progress_bar = tqdm(enumerate(data_loader), total=num_steps, desc=f"Epoch {epoch}/{CFG['EPOCHS']}")
    start_time = time.time()
    epoch_loss = 0.0

    # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•œ ë°°ì¹˜ ì†ì‹¤ ë° ì¸ë±ìŠ¤ ì €ì¥ì†Œ
    accumulated_indices = []
    accumulated_losses = []

    for idx, batch in progress_bar:
        step_start_time = time.time()

        # ë°°ì¹˜ ì–¸íŒ¨í‚¹ (ì¸ë±ìŠ¤ í¬í•¨ ì—¬ë¶€ í™•ì¸)
        if len(batch) == 3:  # ì¸ë±ìŠ¤ í¬í•¨í•˜ëŠ” ê²½ìš°
            samples, targets, indices = batch
        else:
            samples, targets = batch
            indices = None

        if type(samples) == list:
            samples = [item.cuda(non_blocking=True) for item in samples]
        else:
            samples = samples.cuda(non_blocking=True)
        targets = targets.cuda(non_blocking=True)

        if mixup_fn is not None:
            samples, targets = mixup_fn(samples, targets)

        with torch.amp.autocast('cuda', dtype=amp_type):
            outputs = model(samples)
            # Hard negativeë¥¼ ìœ„í•´ ìƒ˜í”Œë³„ ì†ì‹¤ ê³„ì‚°
            if hard_negative_miner is not None:
                sample_losses = nn.CrossEntropyLoss(reduction='none')(outputs['logits'], targets)
                loss = sample_losses.mean()
            else:
                loss = criterion(outputs['logits'], targets)

        if CFG['TRAIN']['ACCUMULATION_STEPS'] > 1:
            loss = loss / CFG['TRAIN']['ACCUMULATION_STEPS']
            
            # Hard negative minerë¥¼ ìœ„í•œ ì •ë³´ ì €ì¥
            if hard_negative_miner is not None and indices is not None:
                accumulated_indices.extend(indices.cpu().numpy())
                accumulated_losses.extend(sample_losses.detach().cpu())
            
            if CFG['AMP_OPT_LEVEL'] != 'O0':
                loss_scaler.scale(loss).backward()
                if (idx + 1) % CFG['TRAIN']['ACCUMULATION_STEPS'] == 0:
                    if CFG['TRAIN']['CLIP_GRAD']:
                        loss_scaler.unscale_(optimizer)
                        grad_norm = torch.nn.utils.clip_grad_norm_(
                            model.parameters(), CFG['TRAIN']['CLIP_GRAD'])
                    loss_scaler.step(optimizer)
                    loss_scaler.update()
                    optimizer.zero_grad()
                    if model_ema is not None:
                        model_ema.update(model)
                    
                    # Hard negative miner ì—…ë°ì´íŠ¸
                    if hard_negative_miner is not None and accumulated_indices:
                        hard_negative_miner.update(accumulated_indices, accumulated_losses)
                        accumulated_indices = []
                        accumulated_losses = []
            else:
                loss.backward()
                if (idx + 1) % CFG['TRAIN']['ACCUMULATION_STEPS'] == 0:
                    if CFG['TRAIN']['CLIP_GRAD']:
                        grad_norm = torch.nn.utils.clip_grad_norm_(
                            model.parameters(), CFG['TRAIN']['CLIP_GRAD'])
                    optimizer.step()
                    optimizer.zero_grad()
                    if model_ema is not None:
                        model_ema.update(model)
                    
                    # Hard negative miner ì—…ë°ì´íŠ¸
                    if hard_negative_miner is not None and accumulated_indices:
                        hard_negative_miner.update(accumulated_indices, accumulated_losses)
                        accumulated_indices = []
                        accumulated_losses = []
        else:
            # Hard negative minerë¥¼ ìœ„í•œ ì •ë³´ ì €ì¥
            if hard_negative_miner is not None and indices is not None:
                accumulated_indices.extend(indices.cpu().numpy())
                accumulated_losses.extend(sample_losses.detach().cpu())
            
            if CFG['AMP_OPT_LEVEL'] != 'O0':
                loss_scaler.scale(loss).backward()
                if CFG['TRAIN']['CLIP_GRAD']:
                    loss_scaler.unscale_(optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        model.parameters(), CFG['TRAIN']['CLIP_GRAD'])
                loss_scaler.step(optimizer)
                loss_scaler.update()
                optimizer.zero_grad()
                if model_ema is not None:
                    model_ema.update(model)
                
                # Hard negative miner ì—…ë°ì´íŠ¸
                if hard_negative_miner is not None and accumulated_indices:
                    hard_negative_miner.update(accumulated_indices, accumulated_losses)
                    accumulated_indices = []
                    accumulated_losses = []
            else:
                loss.backward()
                if CFG['TRAIN']['CLIP_GRAD']:
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        model.parameters(), CFG['TRAIN']['CLIP_GRAD'])
                optimizer.step()
                optimizer.zero_grad()
                if model_ema is not None:
                    model_ema.update(model)
                
                # Hard negative miner ì—…ë°ì´íŠ¸
                if hard_negative_miner is not None and accumulated_indices:
                    hard_negative_miner.update(accumulated_indices, accumulated_losses)
                    accumulated_indices = []
                    accumulated_losses = []

        epoch_loss += loss.item() * targets.size(0)
        elapsed_time = time.time() - start_time
        steps_per_sec = (idx + 1) / elapsed_time
        eta = (num_steps - (idx + 1)) / steps_per_sec
        progress_bar.set_postfix(loss=f"{loss.item():.4f}",
                                 elapsed=f"{time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}",
                                 eta=f"{time.strftime('%H:%M:%S', time.gmtime(eta))}")

        torch.cuda.synchronize()

    lr_scheduler.step() # ì—í­ ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    return epoch_loss / len(data_loader.dataset)

@torch.no_grad()
def validation(CFG, criterion, data_loader, model, device, class_names, epoch=None):
    model.eval()
    total_loss = 0.0
    preds, true_labels = [], []
    total = 0

    progress_bar = tqdm(iter(data_loader), desc="Validation")

    for idx, batch in enumerate(progress_bar):
        # ë°°ì¹˜ ì–¸íŒ¨í‚¹ (ì¸ë±ìŠ¤ í¬í•¨ ì—¬ë¶€ í™•ì¸)
        if len(batch) == 3:  # ì¸ë±ìŠ¤ í¬í•¨í•˜ëŠ” ê²½ìš°
            images, target, _ = batch
        else:
            images, target = batch
        
        if type(images) == list:
            images = [item.float().to(device) for item in images]
        else:
            images = images.float().to(device)
        target = target.to(device)
        output = model(images)
        
        # Hard negativeë¥¼ ìœ„í•´ ìƒ˜í”Œë³„ ì†ì‹¤ ê³„ì‚° í›„ í‰ê· 
        sample_losses = nn.CrossEntropyLoss(reduction='none')(output['logits'], target)
        loss = sample_losses.mean()

        total_loss += loss.item() * target.size(0)
        _, predicted = torch.max(output.logits, 1)
        total += target.size(0)
        preds.extend(predicted.cpu().numpy().tolist())
        true_labels.extend(target.cpu().numpy().tolist())

        progress_bar.set_postfix(loss=f"{loss.item():.4f}")

    _val_loss = total_loss / total
    _val_score = f1_score(true_labels, preds, average='macro', zero_division=0)

    # ğŸ”¹ classë³„ F1 (í´ë˜ìŠ¤ëª… í¬í•¨)
    class_f1 = f1_score(true_labels, preds, average=None, zero_division=0)
    class_f1_dict = {f'{class_names[i]}_f1': f for i, f in enumerate(class_f1)}

    # ğŸ”¹ confusion matrix
    cm = confusion_matrix(true_labels, preds)
    fig, ax = plt.subplots(figsize=(len(class_names), len(class_names)))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
                xticklabels=class_names, yticklabels=class_names)
    ax.set_xlabel("Predicted Labels")
    ax.set_ylabel("True Labels")
    ax.set_title("Confusion Matrix (Epoch {epoch})".format(epoch=epoch) if epoch is not None else "Confusion Matrix")
    plt.tight_layout()

    # ğŸ”¹ wandb ì´ë¯¸ì§€ë¡œ ì €ì¥
    wandb_cm = wandb.Image(fig)
    plt.close(fig)

    return _val_loss, _val_score, class_f1_dict, wandb_cm


def train(CFG, model, criterion, train_loader, val_loader, optimizer, lr_scheduler, scaler, mixup_fn, class_names, model_ema=None, cur_epoch=1, folder_path="./", experiment_name= "model", hard_negative_miner=None):
    best_val_score = 0.0 # Validation ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ë³€ìˆ˜

    for epoch in range(cur_epoch, CFG['EPOCHS']+1):
        train_loss = train_one_epoch(CFG, model, criterion, train_loader, optimizer, epoch, mixup_fn, lr_scheduler, amp_autocast, scaler, model_ema, hard_negative_miner)
        _val_loss, _val_score, class_f1_dict, wandb_cm = validation(CFG, criterion, val_loader, model, device, class_names, epoch)
        print(f"Epoch {epoch}/{CFG['EPOCHS']} - Train Loss: {train_loss:.4f}, Val Loss: {_val_loss:.4f}, macro f1: {_val_score:.2f}%")

        # WandBë¡œ metric ë¡œê¹…
        log_data = {
            'epoch': epoch,
            'train_loss': train_loss,
            'val_loss': _val_loss,
            'val_macro_f1': _val_score,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'confusion_matrix': wandb_cm
        }
        log_data.update(class_f1_dict)
        wandb.log(log_data)

        # Validation ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì €ì¥
        if _val_score > best_val_score:
            best_val_score = _val_score
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': lr_scheduler.state_dict(),
                'best_val_accuracy': best_val_score
            }, os.path.join(folder_path, f"best_{experiment_name}.pth"))

        # ë§ˆì§€ë§‰ ì—í­ ëª¨ë¸ ì €ì¥
        if epoch > 1:
            os.remove(os.path.join(folder_path, f"{experiment_name}-epoch{epoch-1}.pth"))
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': lr_scheduler.state_dict()
        }, os.path.join(folder_path, f"{experiment_name}-epoch{epoch}.pth"))

    wandb.finish()

if __name__ == '__main__':
    import cv2  # PadSquareì™€ CustomDatasetì—ì„œ ì‚¬ìš©
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    trained_path = ''
    model_name = "../../weights/OpenGVLab/internimage_xl_22kto1k_384"
    num_folds = CFG['kFold']  # K-Foldì˜ K ê°’ ì„¤ì •

    le = preprocessing.LabelEncoder()
    all_img_list = glob.glob('../../train/*/*')
    df = pd.DataFrame(columns=['img_path', 'rock_type'])
    df['img_path'] = all_img_list
    df['rock_type'] = df['img_path'].apply(lambda x : str(x).replace('\\','/').split('/')[3])
    df['rock_type'] = le.fit_transform(df['rock_type'])

    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=CFG['SEED'])
    for fold, (train_idx, val_idx) in enumerate(skf.split(df['img_path'], df['rock_type'])):

        trained_path = ""

        if trained_path == "":
            idx = len([x for x in os.listdir('../../experiments') if x.startswith(model_name)])
            experiment_name = f"{os.path.basename(model_name.replace('.','_'))}_fold_{fold+1}" # ì‹¤í—˜ì´ ì €ì¥ë  folder ì´ë¦„
        else:
            experiment_name = os.path.splitext(os.path.basename(trained_path))[0].split('-')[0]
        folder_path = os.path.join("../../experiments", experiment_name)
        wandb.init(
            project="your_project",
            config=CFG,
            name=experiment_name,
            )
        seed_everything(CFG['SEED'])

        train_data = df.iloc[train_idx].copy().reset_index(drop=True)
        val_data = df.iloc[val_idx].copy().reset_index(drop=True)
        
        class_names = le.classes_

        num_classes = len(class_names)

        # ëª¨ë¸ head ë ˆì´ì–´ ì¬ì •ì˜
        model = AutoModelForImageClassification.from_pretrained(model_name, trust_remote_code=True)
        in_features = model.model.head.in_features
        model.model.head = torch.nn.Linear(in_features, num_classes)
        model = model.to(device)

        train_transform = A.Compose([
            RandomCenterCrop(min_size=75, max_size=200, p=0.5),
            A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),
            A.HorizontalFlip(p=0.5),  # 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „
            A.VerticalFlip(p=0.5),    # 50% í™•ë¥ ë¡œ ìƒí•˜ ë°˜ì „
            A.CLAHE(p=0.5),
            A.GaussNoise(std_range=(0.1,0.15), p=0.5),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])
        test_transform = A.Compose([
            RandomCenterCrop(min_size=75, max_size=200, p=0.4),
            A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])

        train_dataset = CustomDataset(train_data['img_path'].values, train_data['rock_type'].values, train_transform)
        hard_negative_miner = HardNegativeMiner(
                    dataset_size=len(train_dataset), 
                    memory_size=CFG['HARD_NEGATIVE_MEMORY_SIZE']
        )
                
        train_loader = create_train_loader_with_accumulation(train_dataset, hard_negative_miner, train_data['rock_type'].values, num_classes, 
                                        CFG['BATCH_SIZE'], CFG['TRAIN']['ACCUMULATION_STEPS'], hard_negative_ratio=0.2)

        val_dataset = CustomDataset(val_data['img_path'].values, val_data['rock_type'].values, test_transform)
        val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=16, pin_memory=True, prefetch_factor=4)

        optimizer = torch.optim.Adam(params=model.parameters(), lr=CFG["LEARNING_RATE"])
        scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=1e-8)
        scaler = GradScaler()

        ###
        config = {'experiment': {}, 'model':{}, 'train':{}, 'validation':{}, 'split':{}, 'seed': {}}
        config['experiment']['name'] = experiment_name

        config['model']['name'] = model_name
        config['model']['IMG_size'] = CFG['IMG_SIZE']

        config['train']['epoch'] = CFG['EPOCHS']
        config['train']['lr'] = CFG['LEARNING_RATE']
        config['train']['train_transform'] = [str(x) for x in train_transform]
        config['train']['optimizer'] = {}
        config['train']['optimizer']['name'] = optimizer.__class__.__name__
        config['train']['scheduler'] = {}
        config['train']['scheduler']['name'] = scheduler.__class__.__name__

        config['validation']['test_transform'] = [str(x) for x in test_transform]

        config['split'] = {}
        config['split']['kFold'] = num_folds

        config['seed'] = CFG['SEED']

        for k, v in optimizer.state_dict()['param_groups'][0].items():
            if k == 'params': continue
            config['train']['optimizer'][k] = v

        for k, v in scheduler.state_dict().items():
            if k == 'params': continue
            config['train']['scheduler'][k] = v
        os.makedirs(folder_path, exist_ok=True)
        config_path = os.path.join(folder_path, "config.json")
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=4)

        ###

        if trained_path != "":
            checkpoint = torch.load(trained_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            cur_epoch = checkpoint['epoch'] + 1
            train(CFG, model, torch.nn.CrossEntropyLoss(), train_loader, val_loader, optimizer, scheduler, scaler, mixup_fn=None, class_names=class_names, cur_epoch=cur_epoch, folder_path=folder_path, experiment_name=experiment_name, hard_negative_miner=hard_negative_miner)

        else:
            train(CFG, model, torch.nn.CrossEntropyLoss(), train_loader, val_loader, optimizer, scheduler, scaler, mixup_fn=None, class_names=class_names, folder_path=folder_path, experiment_name=experiment_name, hard_negative_miner=hard_negative_miner)


        wandb.finish()